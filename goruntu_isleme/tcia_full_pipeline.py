#!/usr/bin/env python3
"""
TCIA Tam Entegrasyon Pipeline'Ä±
==============================

TCIA'dan veri indirme â†’ DICOM iÅŸleme â†’ Model eÄŸitimi â†’ DeÄŸerlendirme
tÃ¼m sÃ¼reci otomatik olarak yÃ¶neten ana script.
"""

import logging
import argparse
from pathlib import Path
from datetime import datetime
import json
import sys

# Proje root'unu Python path'e ekle
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Custom imports
from tcia_data_downloader import TCIADataDownloader
from dicom_processor import DICOMProcessor
from train_tcia_models import TCIAModelTrainer

# Logging ayarla
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tcia_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class TCIAFullPipeline:
    """TCIA tam entegrasyon pipeline'Ä±"""
    
    def __init__(self, 
                 download_dir: str = "tcia_data",
                 processed_dir: str = "processed_dicom", 
                 training_dir: str = "training_dataset",
                 models_dir: str = "models"):
        
        self.download_dir = Path(download_dir)
        self.processed_dir = Path(processed_dir)
        self.training_dir = Path(training_dir)
        self.models_dir = Path(models_dir)
        
        # Pipeline bileÅŸenleri
        self.downloader = TCIADataDownloader(str(self.download_dir))
        self.processor = DICOMProcessor(str(self.processed_dir))
        self.trainer = TCIAModelTrainer(str(self.training_dir), str(self.models_dir))
        
        # Pipeline durumu
        self.pipeline_status = {
            'download_completed': False,
            'processing_completed': False,
            'training_completed': False,
            'evaluation_completed': False
        }
    
    def run_full_pipeline(self, 
                         collections: list = None,
                         max_series_per_collection: int = 10,
                         train_models: bool = True) -> dict:
        """Tam pipeline'Ä± Ã§alÄ±ÅŸtÄ±r"""
        try:
            logger.info("ğŸš€ TCIA Full Pipeline baÅŸlatÄ±lÄ±yor...")
            
            if collections is None:
                collections = ["CPTAC-LUAD", "CMB-BRCA", "UCSF-PDGM"]
            
            pipeline_results = {
                'started_at': datetime.now().isoformat(),
                'collections': collections,
                'max_series_per_collection': max_series_per_collection,
                'steps_completed': [],
                'errors': []
            }
            
            # 1. Veri Ä°ndirme
            try:
                logger.info("ğŸ“¥ AdÄ±m 1: TCIA Verileri Ä°ndiriliyor...")
                download_results = self._download_collections(collections, max_series_per_collection)
                pipeline_results['download_results'] = download_results
                pipeline_results['steps_completed'].append('download')
                self.pipeline_status['download_completed'] = True
                logger.info("âœ… Veri indirme tamamlandÄ±")
                
            except Exception as e:
                error_msg = f"Veri indirme hatasÄ±: {str(e)}"
                logger.error(error_msg)
                pipeline_results['errors'].append(error_msg)
                return pipeline_results
            
            # 2. DICOM Ä°ÅŸleme
            try:
                logger.info("ğŸ”§ AdÄ±m 2: DICOM GÃ¶rÃ¼ntÃ¼leri Ä°ÅŸleniyor...")
                processing_results = self._process_collections(collections)
                pipeline_results['processing_results'] = processing_results
                pipeline_results['steps_completed'].append('processing')
                self.pipeline_status['processing_completed'] = True
                logger.info("âœ… DICOM iÅŸleme tamamlandÄ±")
                
            except Exception as e:
                error_msg = f"DICOM iÅŸleme hatasÄ±: {str(e)}"
                logger.error(error_msg)
                pipeline_results['errors'].append(error_msg)
                return pipeline_results
            
            # 3. EÄŸitim Veri Seti OluÅŸturma
            try:
                logger.info("ğŸ“Š AdÄ±m 3: EÄŸitim Veri Seti OluÅŸturuluyor...")
                dataset_results = self._create_training_dataset()
                pipeline_results['dataset_results'] = dataset_results
                pipeline_results['steps_completed'].append('dataset_creation')
                logger.info("âœ… EÄŸitim veri seti oluÅŸturuldu")
                
            except Exception as e:
                error_msg = f"Veri seti oluÅŸturma hatasÄ±: {str(e)}"
                logger.error(error_msg)
                pipeline_results['errors'].append(error_msg)
                return pipeline_results
            
            # 4. Model EÄŸitimi (opsiyonel)
            if train_models:
                try:
                    logger.info("ğŸ§  AdÄ±m 4: Modeller EÄŸitiliyor...")
                    training_results = self._train_models()
                    pipeline_results['training_results'] = training_results
                    pipeline_results['steps_completed'].append('training')
                    self.pipeline_status['training_completed'] = True
                    logger.info("âœ… Model eÄŸitimi tamamlandÄ±")
                    
                except Exception as e:
                    error_msg = f"Model eÄŸitimi hatasÄ±: {str(e)}"
                    logger.error(error_msg)
                    pipeline_results['errors'].append(error_msg)
            
            # 5. Pipeline Ã–zeti
            pipeline_results['completed_at'] = datetime.now().isoformat()
            pipeline_results['pipeline_status'] = self.pipeline_status
            
            # SonuÃ§larÄ± kaydet
            self._save_pipeline_results(pipeline_results)
            
            logger.info("ğŸ‰ TCIA Full Pipeline tamamlandÄ±!")
            return pipeline_results
            
        except Exception as e:
            logger.error(f"Pipeline hatasÄ±: {str(e)}")
            raise
    
    def _download_collections(self, collections: list, max_series: int) -> dict:
        """KoleksiyonlarÄ± indir"""
        download_results = {}
        
        for collection_id in collections:
            try:
                logger.info(f"Koleksiyon indiriliyor: {collection_id}")
                result = self.downloader.download_collection_sample(
                    collection_id, max_series_per_collection=max_series
                )
                download_results[collection_id] = result
                
            except Exception as e:
                logger.error(f"Koleksiyon indirme hatasÄ± ({collection_id}): {str(e)}")
                download_results[collection_id] = {'error': str(e)}
        
        return download_results
    
    def _process_collections(self, collections: list) -> dict:
        """KoleksiyonlarÄ± iÅŸle"""
        processing_results = {}
        
        for collection_id in collections:
            try:
                collection_dir = self.download_dir / collection_id
                if collection_dir.exists():
                    logger.info(f"Koleksiyon iÅŸleniyor: {collection_id}")
                    result = self.processor.batch_process_collection(str(collection_dir))
                    processing_results[collection_id] = result
                else:
                    logger.warning(f"Koleksiyon klasÃ¶rÃ¼ bulunamadÄ±: {collection_dir}")
                    processing_results[collection_id] = {'error': 'Collection directory not found'}
                    
            except Exception as e:
                logger.error(f"Koleksiyon iÅŸleme hatasÄ± ({collection_id}): {str(e)}")
                processing_results[collection_id] = {'error': str(e)}
        
        return processing_results
    
    def _create_training_dataset(self) -> dict:
        """EÄŸitim veri seti oluÅŸtur"""
        try:
            # Ä°ÅŸlenmiÅŸ koleksiyon klasÃ¶rlerini bul
            processed_dirs = []
            for item in self.processed_dir.iterdir():
                if item.is_dir() and any(col in item.name for col in ["CPTAC", "CMB", "UCSF"]):
                    processed_dirs.append(str(item))
            
            if not processed_dirs:
                raise ValueError("Ä°ÅŸlenmiÅŸ DICOM klasÃ¶rÃ¼ bulunamadÄ±")
            
            # EÄŸitim veri seti oluÅŸtur
            dataset_result = self.processor.create_training_dataset(
                processed_dirs, str(self.training_dir)
            )
            
            return dataset_result
            
        except Exception as e:
            logger.error(f"EÄŸitim veri seti oluÅŸturma hatasÄ±: {str(e)}")
            raise
    
    def _train_models(self) -> dict:
        """Modelleri eÄŸit"""
        try:
            models, results = self.trainer.train_all_models()
            
            training_results = {
                'trained_models': list(models.keys()),
                'model_results': results,
                'training_completed': True
            }
            
            return training_results
            
        except Exception as e:
            logger.error(f"Model eÄŸitimi hatasÄ±: {str(e)}")
            raise
    
    def _save_pipeline_results(self, results: dict):
        """Pipeline sonuÃ§larÄ±nÄ± kaydet"""
        try:
            results_file = Path("tcia_pipeline_results.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Pipeline sonuÃ§larÄ± kaydedildi: {results_file}")
            
        except Exception as e:
            logger.error(f"SonuÃ§ kaydetme hatasÄ±: {str(e)}")
    
    def get_pipeline_status(self) -> dict:
        """Pipeline durumunu getir"""
        return {
            'pipeline_status': self.pipeline_status,
            'directories': {
                'download_dir': str(self.download_dir),
                'processed_dir': str(self.processed_dir),
                'training_dir': str(self.training_dir),
                'models_dir': str(self.models_dir)
            }
        }
    
    def cleanup_intermediate_files(self):
        """Ara dosyalarÄ± temizle"""
        try:
            logger.info("Ara dosyalar temizleniyor...")
            
            # DICOM iÅŸleme ara dosyalarÄ±nÄ± temizle
            if self.processed_dir.exists():
                import shutil
                shutil.rmtree(self.processed_dir)
                logger.info("Ä°ÅŸlenmiÅŸ DICOM dosyalarÄ± temizlendi")
            
            logger.info("Temizlik tamamlandÄ±")
            
        except Exception as e:
            logger.error(f"Temizlik hatasÄ±: {str(e)}")


def main():
    """Ana fonksiyon"""
    parser = argparse.ArgumentParser(description="TCIA Full Pipeline")
    parser.add_argument("--collections", nargs="+", 
                       default=["CPTAC-LUAD", "CMB-BRCA", "UCSF-PDGM"],
                       help="Ä°ndirilecek koleksiyonlar")
    parser.add_argument("--max-series", type=int, default=10,
                       help="Koleksiyon baÅŸÄ±na maksimum seri sayÄ±sÄ±")
    parser.add_argument("--no-training", action="store_true",
                       help="Model eÄŸitimi yapma")
    parser.add_argument("--cleanup", action="store_true",
                       help="Ara dosyalarÄ± temizle")
    
    args = parser.parse_args()
    
    # Pipeline oluÅŸtur
    pipeline = TCIAFullPipeline()
    
    try:
        if args.cleanup:
            pipeline.cleanup_intermediate_files()
            return
        
        # Pipeline durumunu gÃ¶ster
        status = pipeline.get_pipeline_status()
        print("ğŸ” Pipeline Durumu:")
        print(f"  â€¢ Ä°ndirme KlasÃ¶rÃ¼: {status['directories']['download_dir']}")
        print(f"  â€¢ Ä°ÅŸleme KlasÃ¶rÃ¼: {status['directories']['processed_dir']}")
        print(f"  â€¢ EÄŸitim KlasÃ¶rÃ¼: {status['directories']['training_dir']}")
        print(f"  â€¢ Modeller KlasÃ¶rÃ¼: {status['directories']['models_dir']}")
        
        # Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r
        results = pipeline.run_full_pipeline(
            collections=args.collections,
            max_series_per_collection=args.max_series,
            train_models=not args.no_training
        )
        
        # SonuÃ§larÄ± gÃ¶ster
        print("\nğŸ“Š Pipeline SonuÃ§larÄ±:")
        print(f"  â€¢ Tamamlanan AdÄ±mlar: {', '.join(results['steps_completed'])}")
        
        if 'download_results' in results:
            total_downloaded = sum(
                r.get('downloaded_series', 0) 
                for r in results['download_results'].values() 
                if 'downloaded_series' in r
            )
            print(f"  â€¢ Ä°ndirilen Seri SayÄ±sÄ±: {total_downloaded}")
        
        if 'dataset_results' in results:
            print(f"  â€¢ Toplam GÃ¶rÃ¼ntÃ¼: {results['dataset_results']['total_images']}")
            print(f"  â€¢ EÄŸitim GÃ¶rÃ¼ntÃ¼leri: {results['dataset_results']['train_images']}")
            print(f"  â€¢ Validasyon GÃ¶rÃ¼ntÃ¼leri: {results['dataset_results']['val_images']}")
            print(f"  â€¢ Test GÃ¶rÃ¼ntÃ¼leri: {results['dataset_results']['test_images']}")
        
        if 'training_results' in results:
            print(f"  â€¢ EÄŸitilen Modeller: {len(results['training_results']['trained_models'])}")
            for model_name, result in results['training_results']['model_results'].items():
                print(f"    - {model_name}: {result['accuracy']:.2f}% accuracy")
        
        if results['errors']:
            print(f"\nâš ï¸ Hatalar:")
            for error in results['errors']:
                print(f"  â€¢ {error}")
        
        print(f"\nâœ… Pipeline tamamlandÄ±: {results['completed_at']}")
        
    except Exception as e:
        print(f"âŒ Pipeline hatasÄ±: {str(e)}")
        logger.error(f"Pipeline hatasÄ±: {str(e)}")


if __name__ == "__main__":
    main()
