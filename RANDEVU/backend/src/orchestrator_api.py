#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FastAPI Orchestrator
OpenAI uyumlu API endpoint'i
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import logging
import sys
import os
from datetime import datetime

# Kendi modÃ¼llerimizi import et
from llm_client import LLMClient
from classify_core import predict_clinic, classifier

# Logging ayarla
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# FastAPI app oluÅŸtur
app = FastAPI(
    title="TanÄ±AI Klinik Ã–neri API",
    description="Hasta ÅŸikayetlerini analiz ederek uygun kliniÄŸi Ã¶neren AI sistemi",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# LLM Client
llm_client = LLMClient()

# Pydantic modelleri
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "clinic-recommender"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.1
    max_tokens: Optional[int] = 200

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[dict]
    usage: dict

class ClinicRecommendation(BaseModel):
    recommended_clinic: str
    confidence: float
    reasoning: str
    alternatives: List[str]
    timestamp: str

# Startup event
@app.on_event("startup")
async def startup_event():
    """Uygulama baÅŸlangÄ±cÄ±nda modeli yÃ¼kle"""
    logger.info("TanÄ±AI Klinik Ã–neri API baÅŸlatÄ±lÄ±yor...")
    
    # Modeli yÃ¼kle
    if not classifier.load_model():
        logger.error("Model yÃ¼klenemedi! Uygulama baÅŸlatÄ±lamÄ±yor.")
        sys.exit(1)
    
    # LLM baÄŸlantÄ±sÄ±nÄ± test et
    if not llm_client.test_connection():
        logger.warning("LiteLLM baÄŸlantÄ±sÄ± kurulamadÄ±! Normalizasyon Ã§alÄ±ÅŸmayabilir.")
    
    logger.info("API baÅŸarÄ±yla baÅŸlatÄ±ldÄ±!")

@app.get("/")
async def root():
    """Ana endpoint"""
    return {
        "message": "TanÄ±AI Klinik Ã–neri API",
        "version": "1.0.0",
        "status": "active"
    }

@app.get("/health")
async def health_check():
    """SaÄŸlÄ±k kontrolÃ¼"""
    model_loaded = classifier.model is not None
    llm_connected = llm_client.test_connection()
    
    return {
        "status": "healthy" if model_loaded else "unhealthy",
        "model_loaded": model_loaded,
        "llm_connected": llm_connected,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI uyumlu chat completions endpoint
    """
    try:
        # Son kullanÄ±cÄ± mesajÄ±nÄ± al
        user_message = None
        for message in reversed(request.messages):
            if message.role == "user":
                user_message = message.content
                break
        
        if not user_message:
            raise HTTPException(status_code=400, detail="KullanÄ±cÄ± mesajÄ± bulunamadÄ±")
        
        logger.info(f"Gelen ÅŸikayet: {user_message}")
        
        # 1. LLM ile metni normalize et
        normalized_text = llm_client.normalize_text(user_message)
        logger.info(f"Normalize edilmiÅŸ: {normalized_text}")
        
        # 2. SVM modeli ile klinik tahmini yap
        try:
            clinic = predict_clinic(normalized_text)
        except Exception as e:
            logger.error(f"Klinik tahmin hatasÄ±: {e}")
            raise HTTPException(status_code=500, detail=f"Klinik tahmini yapÄ±lamadÄ±: {str(e)}")
        
        # 3. YanÄ±t oluÅŸtur
        reasoning = f"Åikayetleriniz analiz edildi ve {clinic} bÃ¶lÃ¼mÃ¼ne yÃ¶nlendiriliyorsunuz."
        
        # Alternatif klinikler (basit Ã¶rnek)
        alternatives = ["Aile HekimliÄŸi", "Ä°Ã§ HastalÄ±klarÄ± (Dahiliye)"]
        if clinic in alternatives:
            alternatives.remove(clinic)
        
        # OpenAI formatÄ±nda yanÄ±t
        response_content = f"""ğŸ¥ **Klinik Ã–nerisi**

**Ã–nerilen Klinik:** {clinic}
**AÃ§Ä±klama:** {reasoning}

**Alternatif SeÃ§enekler:**
{chr(10).join([f"â€¢ {alt}" for alt in alternatives[:3]])}

**Not:** Bu Ã¶neri AI sistemi tarafÄ±ndan oluÅŸturulmuÅŸtur. Kesin tanÄ± iÃ§in mutlaka doktor muayenesi gereklidir."""

        # OpenAI formatÄ±nda response
        response = ChatCompletionResponse(
            id=f"chatcmpl-{datetime.now().strftime('%Y%m%d%H%M%S')}",
            created=int(datetime.now().timestamp()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_content
                },
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(user_message.split()),
                "completion_tokens": len(response_content.split()),
                "total_tokens": len(user_message.split()) + len(response_content.split())
            }
        )
        
        logger.info(f"YanÄ±t oluÅŸturuldu: {clinic}")
        
        return response
        
    except Exception as e:
        logger.error(f"API hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=f"Sunucu hatasÄ±: {str(e)}")

@app.post("/recommend-clinic", response_model=ClinicRecommendation)
async def recommend_clinic(request: dict):
    """
    Basit klinik Ã¶nerisi endpoint'i
    """
    try:
        symptoms = request.get("symptoms", "")
        if not symptoms:
            raise HTTPException(status_code=400, detail="Åikayet metni gerekli")
        
        # Normalize et
        normalized_text = llm_client.normalize_text(symptoms)
        
        # Tahmin yap
        try:
            clinic = predict_clinic(normalized_text)
        except Exception as e:
            logger.error(f"Klinik tahmin hatasÄ±: {e}")
            raise HTTPException(status_code=500, detail=f"Klinik tahmini yapÄ±lamadÄ±: {str(e)}")
        
        # YanÄ±t oluÅŸtur
        
        reasoning = f"Åikayetleriniz analiz edildi ve {clinic} bÃ¶lÃ¼mÃ¼ne yÃ¶nlendiriliyorsunuz."
        
        return ClinicRecommendation(
            recommended_clinic=clinic,
            confidence=0.95,  # Sabit gÃ¼ven deÄŸeri
            reasoning=reasoning,
            alternatives=["Aile HekimliÄŸi", "Ä°Ã§ HastalÄ±klarÄ± (Dahiliye)"],
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"Recommendation hatasÄ±: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/clinics")
async def get_available_clinics():
    """Mevcut klinikleri listele"""
    clinics = classifier.get_available_clinics()
    return {
        "clinics": clinics,
        "count": len(clinics)
    }

@app.get("/model-info")
async def get_model_info():
    """Model bilgilerini getir"""
    return classifier.get_model_info()

if __name__ == "__main__":
    import uvicorn
    import os
    uvicorn.run(
        app, 
        host=os.getenv("HOST", "0.0.0.0"), 
        port=int(os.getenv("PORT", "8001"))
    )
