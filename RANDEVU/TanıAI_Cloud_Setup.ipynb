{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¥ TanÄ±AI Cloud Kurulumu\n",
        "\n",
        "Google Colab/Kaggle ortamÄ±nda Whisper + ML + LLM entegre sistemi\n",
        "\n",
        "## ğŸ¯ Ã–zellikler\n",
        "- **Whisper Medium Model**: Ses dosyalarÄ±nÄ± TÃ¼rkÃ§e metne Ã§evirir\n",
        "- **ML Triage Model**: Scikit-learn tabanlÄ± klinik Ã¶neri sistemi\n",
        "- **LLM Entegrasyonu**: Ollama + kÃ¼Ã§Ã¼k modeller (0.5B-2B parametre)\n",
        "- **Hibrit Sistem**: ML + LLM sonuÃ§larÄ±nÄ± birleÅŸtirerek daha doÄŸru Ã¶neriler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ 1. Gerekli Paketleri Kur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerekli paketleri kur\n",
        "%pip install openai-whisper torch torchaudio scikit-learn pandas numpy fastapi uvicorn python-multipart requests aiohttp\n",
        "\n",
        "print(\"âœ… TÃ¼m paketler kuruldu!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– 2. Ollama Kurulumu (LLM iÃ§in)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama'yÄ± kur\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "\n",
        "print(\"âœ… Ollama kuruldu!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama servisini baÅŸlat (arka planda)\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Ollama servisini baÅŸlat\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
        "                                 stdout=subprocess.DEVNULL, \n",
        "                                 stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Servisin baÅŸlamasÄ±nÄ± bekle\n",
        "time.sleep(5)\n",
        "\n",
        "print(\"âœ… Ollama servisi baÅŸlatÄ±ldÄ±!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¥ 3. KÃ¼Ã§Ã¼k Model Ä°ndir (2B Parametre)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KÃ¼Ã§Ã¼k model indir (2B parametre)\n",
        "!ollama pull llama2:2b\n",
        "\n",
        "print(\"âœ… Llama2:2b modeli indirildi!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª 4. Ollama Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama modelini test et\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def test_ollama():\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": \"llama2:2b\",\n",
        "        \"prompt\": \"Merhaba, nasÄ±lsÄ±n?\",\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            print(\"âœ… Ollama Ã§alÄ±ÅŸÄ±yor!\")\n",
        "            print(f\"YanÄ±t: {result.get('response', '')[:100]}...\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ Ollama hatasÄ±: {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ollama baÄŸlantÄ± hatasÄ±: {e}\")\n",
        "        return False\n",
        "\n",
        "test_ollama()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ 5. Proje DosyalarÄ±nÄ± YÃ¼kle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Proje dosyalarÄ±nÄ± yÃ¼kle\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“ Proje dosyalarÄ±nÄ± yÃ¼kleyin (zip formatÄ±nda)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Zip dosyasÄ±nÄ± Ã§Ä±kar\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "        print(f\"âœ… {filename} Ã§Ä±karÄ±ldÄ±\")\n",
        "        break\n",
        "\n",
        "print(\"âœ… Proje dosyalarÄ± hazÄ±r!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª 6. Sistem Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ML modelini test et\n",
        "import sys\n",
        "sys.path.append('/content/RANDEVU')\n",
        "\n",
        "try:\n",
        "    from ml_clinic.triage_model import get_triage_model\n",
        "    \n",
        "    model = get_triage_model()\n",
        "    result = model.suggest(\"BaÅŸ aÄŸrÄ±m var ve mide bulantÄ±sÄ± yaÅŸÄ±yorum\", top_k=3)\n",
        "    \n",
        "    print(\"âœ… ML Model Ã§alÄ±ÅŸÄ±yor!\")\n",
        "    print(f\"Ã–neriler: {result.get('suggestions', [])}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ ML Model hatasÄ±: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whisper modelini test et\n",
        "import whisper\n",
        "\n",
        "try:\n",
        "    # Medium model kullan (cloud iÃ§in optimize)\n",
        "    model = whisper.load_model(\"medium\")\n",
        "    print(\"âœ… Whisper Medium modeli yÃ¼klendi!\")\n",
        "    print(f\"Model boyutu: {model.dims}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Whisper hatasÄ±: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– 7. Cloud LLM Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cloud LLM'i test et\n",
        "try:\n",
        "    from cloud_llm_analyzer import CloudLLMAnalyzer, CloudLLMConfig\n",
        "    \n",
        "    config = CloudLLMConfig(\n",
        "        provider=\"ollama\",\n",
        "        model=\"llama2:2b\",\n",
        "        base_url=\"http://localhost:11434\"\n",
        "    )\n",
        "    \n",
        "    analyzer = CloudLLMAnalyzer(config)\n",
        "    \n",
        "    # Test metni\n",
        "    test_text = \"BaÅŸ aÄŸrÄ±m var ve mide bulantÄ±sÄ± yaÅŸÄ±yorum\"\n",
        "    result = analyzer.analyze_with_cloud_llm(test_text)\n",
        "    \n",
        "    print(\"âœ… Cloud LLM Ã§alÄ±ÅŸÄ±yor!\")\n",
        "    print(f\"Test: {test_text}\")\n",
        "    print(f\"Ã–neri: {result.get('clinic', 'Bilinmeyen')}\")\n",
        "    print(f\"GÃ¼ven: {result.get('confidence', 0):.2f}\")\n",
        "    print(f\"AÃ§Ä±klama: {result.get('reasoning', '')}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Cloud LLM hatasÄ±: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ 8. Entegre Sistem Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entegre sistemi test et\n",
        "try:\n",
        "    from ml_clinic.integrated_triage import get_integrated_triage, TriageConfig\n",
        "    from ml_clinic.llm_clinic_analyzer import LLMConfig\n",
        "    \n",
        "    # Config'leri ayarla\n",
        "    llm_config = LLMConfig(\n",
        "        provider=\"ollama\",\n",
        "        model=\"llama2:2b\",\n",
        "        base_url=\"http://localhost:11434\"\n",
        "    )\n",
        "    \n",
        "    triage_config = TriageConfig(\n",
        "        use_ml=True,\n",
        "        use_llm=True,\n",
        "        ml_weight=0.6,\n",
        "        llm_weight=0.4\n",
        "    )\n",
        "    \n",
        "    # Entegre sistemi baÅŸlat\n",
        "    integrated_triage = get_integrated_triage(triage_config, llm_config)\n",
        "    \n",
        "    # Test senaryolarÄ±\n",
        "    test_cases = [\n",
        "        \"BaÅŸ aÄŸrÄ±m var ve mide bulantÄ±sÄ± yaÅŸÄ±yorum\",\n",
        "        \"Ã‡ocuÄŸumda ateÅŸ ve Ã¶ksÃ¼rÃ¼k var\",\n",
        "        \"GÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± ve nefes darlÄ±ÄŸÄ± Ã§ekiyorum\",\n",
        "        \"DiÅŸ aÄŸrÄ±sÄ± dayanÄ±lmaz halde\"\n",
        "    ]\n",
        "    \n",
        "    print(\"âœ… Entegre sistem Ã§alÄ±ÅŸÄ±yor!\")\n",
        "    print(\"\\nğŸ“ Test SonuÃ§larÄ±:\")\n",
        "    \n",
        "    for i, test_text in enumerate(test_cases, 1):\n",
        "        result = integrated_triage.suggest(test_text, top_k=3)\n",
        "        print(f\"\\n{i}. {test_text}\")\n",
        "        print(f\"   Ã–neriler: {result.get('suggestions', [])}\")\n",
        "        print(f\"   YÃ¶ntemler: {result.get('methods_used', [])}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Entegre sistem hatasÄ±: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
