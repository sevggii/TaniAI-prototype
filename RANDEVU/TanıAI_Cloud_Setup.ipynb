{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🏥 TanıAI Cloud Kurulumu\n",
        "\n",
        "Google Colab/Kaggle ortamında Whisper + ML + LLM entegre sistemi\n",
        "\n",
        "## 🎯 Özellikler\n",
        "- **Whisper Medium Model**: Ses dosyalarını Türkçe metne çevirir\n",
        "- **ML Triage Model**: Scikit-learn tabanlı klinik öneri sistemi\n",
        "- **LLM Entegrasyonu**: Ollama + küçük modeller (0.5B-2B parametre)\n",
        "- **Hibrit Sistem**: ML + LLM sonuçlarını birleştirerek daha doğru öneriler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📦 1. Gerekli Paketleri Kur\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerekli paketleri kur\n",
        "%pip install openai-whisper torch torchaudio scikit-learn pandas numpy fastapi uvicorn python-multipart requests aiohttp\n",
        "\n",
        "print(\"✅ Tüm paketler kuruldu!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 2. Ollama Kurulumu (LLM için)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama'yı kur\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "\n",
        "print(\"✅ Ollama kuruldu!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama servisini başlat (arka planda)\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Ollama servisini başlat\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], \n",
        "                                 stdout=subprocess.DEVNULL, \n",
        "                                 stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Servisin başlamasını bekle\n",
        "time.sleep(5)\n",
        "\n",
        "print(\"✅ Ollama servisi başlatıldı!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📥 3. Küçük Model İndir (2B Parametre)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Küçük model indir (2B parametre)\n",
        "!ollama pull llama2:2b\n",
        "\n",
        "print(\"✅ Llama2:2b modeli indirildi!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 4. Ollama Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ollama modelini test et\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def test_ollama():\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": \"llama2:2b\",\n",
        "        \"prompt\": \"Merhaba, nasılsın?\",\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, json=payload, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            print(\"✅ Ollama çalışıyor!\")\n",
        "            print(f\"Yanıt: {result.get('response', '')[:100]}...\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"❌ Ollama hatası: {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama bağlantı hatası: {e}\")\n",
        "        return False\n",
        "\n",
        "test_ollama()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📁 5. Proje Dosyalarını Yükle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Proje dosyalarını yükle\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"📁 Proje dosyalarını yükleyin (zip formatında)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Zip dosyasını çıkar\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "        print(f\"✅ {filename} çıkarıldı\")\n",
        "        break\n",
        "\n",
        "print(\"✅ Proje dosyaları hazır!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 6. Sistem Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ML modelini test et\n",
        "import sys\n",
        "sys.path.append('/content/RANDEVU')\n",
        "\n",
        "try:\n",
        "    from ml_clinic.triage_model import get_triage_model\n",
        "    \n",
        "    model = get_triage_model()\n",
        "    result = model.suggest(\"Baş ağrım var ve mide bulantısı yaşıyorum\", top_k=3)\n",
        "    \n",
        "    print(\"✅ ML Model çalışıyor!\")\n",
        "    print(f\"Öneriler: {result.get('suggestions', [])}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ ML Model hatası: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whisper modelini test et\n",
        "import whisper\n",
        "\n",
        "try:\n",
        "    # Medium model kullan (cloud için optimize)\n",
        "    model = whisper.load_model(\"medium\")\n",
        "    print(\"✅ Whisper Medium modeli yüklendi!\")\n",
        "    print(f\"Model boyutu: {model.dims}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Whisper hatası: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 7. Cloud LLM Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cloud LLM'i test et\n",
        "try:\n",
        "    from cloud_llm_analyzer import CloudLLMAnalyzer, CloudLLMConfig\n",
        "    \n",
        "    config = CloudLLMConfig(\n",
        "        provider=\"ollama\",\n",
        "        model=\"llama2:2b\",\n",
        "        base_url=\"http://localhost:11434\"\n",
        "    )\n",
        "    \n",
        "    analyzer = CloudLLMAnalyzer(config)\n",
        "    \n",
        "    # Test metni\n",
        "    test_text = \"Baş ağrım var ve mide bulantısı yaşıyorum\"\n",
        "    result = analyzer.analyze_with_cloud_llm(test_text)\n",
        "    \n",
        "    print(\"✅ Cloud LLM çalışıyor!\")\n",
        "    print(f\"Test: {test_text}\")\n",
        "    print(f\"Öneri: {result.get('clinic', 'Bilinmeyen')}\")\n",
        "    print(f\"Güven: {result.get('confidence', 0):.2f}\")\n",
        "    print(f\"Açıklama: {result.get('reasoning', '')}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Cloud LLM hatası: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔄 8. Entegre Sistem Testi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entegre sistemi test et\n",
        "try:\n",
        "    from ml_clinic.integrated_triage import get_integrated_triage, TriageConfig\n",
        "    from ml_clinic.llm_clinic_analyzer import LLMConfig\n",
        "    \n",
        "    # Config'leri ayarla\n",
        "    llm_config = LLMConfig(\n",
        "        provider=\"ollama\",\n",
        "        model=\"llama2:2b\",\n",
        "        base_url=\"http://localhost:11434\"\n",
        "    )\n",
        "    \n",
        "    triage_config = TriageConfig(\n",
        "        use_ml=True,\n",
        "        use_llm=True,\n",
        "        ml_weight=0.6,\n",
        "        llm_weight=0.4\n",
        "    )\n",
        "    \n",
        "    # Entegre sistemi başlat\n",
        "    integrated_triage = get_integrated_triage(triage_config, llm_config)\n",
        "    \n",
        "    # Test senaryoları\n",
        "    test_cases = [\n",
        "        \"Baş ağrım var ve mide bulantısı yaşıyorum\",\n",
        "        \"Çocuğumda ateş ve öksürük var\",\n",
        "        \"Göğüs ağrısı ve nefes darlığı çekiyorum\",\n",
        "        \"Diş ağrısı dayanılmaz halde\"\n",
        "    ]\n",
        "    \n",
        "    print(\"✅ Entegre sistem çalışıyor!\")\n",
        "    print(\"\\n📝 Test Sonuçları:\")\n",
        "    \n",
        "    for i, test_text in enumerate(test_cases, 1):\n",
        "        result = integrated_triage.suggest(test_text, top_k=3)\n",
        "        print(f\"\\n{i}. {test_text}\")\n",
        "        print(f\"   Öneriler: {result.get('suggestions', [])}\")\n",
        "        print(f\"   Yöntemler: {result.get('methods_used', [])}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Entegre sistem hatası: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
