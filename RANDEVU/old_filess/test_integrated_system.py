#!/usr/bin/env python3
"""
Entegre Sistem Test Scripti
Whisper + ML + LLM sistemini test eder
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# Proje kÃ¶k dizinini path'e ekle
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ModÃ¼lleri import et
try:
    from ml_clinic.integrated_triage import get_integrated_triage, TriageConfig
    from ml_clinic.llm_clinic_analyzer import LLMConfig
    from whisper_asr import get_asr_processor
    print("âœ… TÃ¼m modÃ¼ller baÅŸarÄ±yla import edildi")
except ImportError as e:
    print(f"âŒ ModÃ¼l import hatasÄ±: {e}")
    sys.exit(1)

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_ml_model():
    """ML modelini test et"""
    print("\nğŸ” ML Model Testi...")
    try:
        from ml_clinic.triage_model import get_triage_model
        ml_model = get_triage_model()
        
        test_text = "BaÅŸ aÄŸrÄ±m var ve mide bulantÄ±sÄ± yaÅŸÄ±yorum"
        result = ml_model.suggest(test_text, top_k=3)
        
        print(f"âœ… ML Model Ã§alÄ±ÅŸÄ±yor")
        print(f"   Test metni: {test_text}")
        print(f"   Ã–neriler: {result.get('suggestions', [])}")
        return True
    except Exception as e:
        print(f"âŒ ML Model hatasÄ±: {e}")
        return False

def test_llm_availability():
    """LLM eriÅŸilebilirliÄŸini test et"""
    print("\nğŸ” LLM EriÅŸilebilirlik Testi...")
    try:
        llm_config = LLMConfig()
        from ml_clinic.llm_clinic_analyzer import get_llm_analyzer
        llm_analyzer = get_llm_analyzer(llm_config)
        
        availability = llm_analyzer.check_llm_availability()
        
        if availability.get("available", False):
            print(f"âœ… LLM eriÅŸilebilir")
            print(f"   Provider: {availability.get('provider', 'Bilinmeyen')}")
            print(f"   Model: {availability.get('current_model', 'Bilinmeyen')}")
            if "models" in availability:
                print(f"   Mevcut modeller: {availability['models'][:3]}...")
            return True
        else:
            print(f"âš ï¸ LLM eriÅŸilemez: {availability.get('error', 'Bilinmeyen hata')}")
            print("   Ollama kurulu deÄŸil veya Ã§alÄ±ÅŸmÄ±yor olabilir")
            return False
    except Exception as e:
        print(f"âŒ LLM test hatasÄ±: {e}")
        return False

def test_whisper():
    """Whisper modelini test et"""
    print("\nğŸ” Whisper Model Testi...")
    try:
        asr_processor = get_asr_processor()
        
        print(f"âœ… Whisper model yÃ¼klendi")
        print(f"   Model: {asr_processor.model_name}")
        print(f"   Model yÃ¼klÃ¼: {asr_processor.model is not None}")
        return True
    except Exception as e:
        print(f"âŒ Whisper test hatasÄ±: {e}")
        return False

def test_integrated_system():
    """Entegre sistemi test et"""
    print("\nğŸ” Entegre Sistem Testi...")
    try:
        # LLM config'i oluÅŸtur
        llm_config = LLMConfig(
            provider="ollama",
            model="llama2:7b",  # veya mevcut model
            base_url="http://localhost:11434"
        )
        
        # Triage config'i oluÅŸtur
        triage_config = TriageConfig(
            use_ml=True,
            use_llm=True,
            ml_weight=0.6,
            llm_weight=0.4
        )
        
        # Entegre sistemi baÅŸlat
        integrated_triage = get_integrated_triage(triage_config, llm_config)
        
        # Sistem durumunu kontrol et
        status = integrated_triage.get_system_status()
        print(f"âœ… Entegre sistem baÅŸlatÄ±ldÄ±")
        print(f"   ML kullanÄ±labilir: {status.get('ml_available', False)}")
        print(f"   LLM kullanÄ±labilir: {status.get('llm_available', False)}")
        print(f"   LLM Provider: {status.get('llm_provider', 'Bilinmeyen')}")
        print(f"   LLM Model: {status.get('llm_model', 'Bilinmeyen')}")
        
        # Test metni ile analiz yap
        test_cases = [
            "BaÅŸ aÄŸrÄ±m var ve mide bulantÄ±sÄ± yaÅŸÄ±yorum",
            "Ã‡ocuÄŸumda ateÅŸ ve Ã¶ksÃ¼rÃ¼k var",
            "GÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± ve nefes darlÄ±ÄŸÄ± Ã§ekiyorum",
            "DiÅŸ aÄŸrÄ±sÄ± dayanÄ±lmaz halde"
        ]
        
        print(f"\nğŸ“ Test Analizleri:")
        for i, test_text in enumerate(test_cases, 1):
            print(f"\n   Test {i}: {test_text}")
            try:
                result = integrated_triage.suggest(test_text, top_k=3)
                print(f"   âœ… Ã–neriler: {result.get('suggestions', [])}")
                print(f"   ğŸ“Š KullanÄ±lan yÃ¶ntemler: {result.get('methods_used', [])}")
                if 'confidence_scores' in result:
                    print(f"   ğŸ¯ GÃ¼ven skorlarÄ±: {result['confidence_scores']}")
            except Exception as e:
                print(f"   âŒ Analiz hatasÄ±: {e}")
        
        return True
    except Exception as e:
        print(f"âŒ Entegre sistem test hatasÄ±: {e}")
        return False

def test_clinic_dataset():
    """Klinik dataset'ini test et"""
    print("\nğŸ” Klinik Dataset Testi...")
    try:
        dataset_path = project_root / "ml_clinic" / "klinik_dataset.jsonl"
        
        if not dataset_path.exists():
            print(f"âŒ Dataset dosyasÄ± bulunamadÄ±: {dataset_path}")
            return False
        
        # Ä°lk birkaÃ§ satÄ±rÄ± oku
        with open(dataset_path, 'r', encoding='utf-8') as f:
            lines = [f.readline().strip() for _ in range(5)]
        
        print(f"âœ… Dataset dosyasÄ± mevcut")
        print(f"   Dosya boyutu: {dataset_path.stat().st_size / 1024 / 1024:.1f} MB")
        print(f"   Ä°lk Ã¶rnekler:")
        for i, line in enumerate(lines, 1):
            if line:
                import json
                try:
                    data = json.loads(line)
                    print(f"     {i}. {data.get('complaint', '')[:50]}... -> {data.get('clinic', '')}")
                except:
                    print(f"     {i}. {line[:50]}...")
        
        return True
    except Exception as e:
        print(f"âŒ Dataset test hatasÄ±: {e}")
        return False

def main():
    """Ana test fonksiyonu"""
    print("ğŸš€ TanÄ±AI Entegre Sistem Testi BaÅŸlÄ±yor...")
    print("=" * 60)
    
    tests = [
        ("Klinik Dataset", test_clinic_dataset),
        ("ML Model", test_ml_model),
        ("Whisper Model", test_whisper),
        ("LLM EriÅŸilebilirlik", test_llm_availability),
        ("Entegre Sistem", test_integrated_system),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ {test_name} testi beklenmeyen hata: {e}")
            results[test_name] = False
    
    # SonuÃ§larÄ± Ã¶zetle
    print("\n" + "=" * 60)
    print("ğŸ“Š TEST SONUÃ‡LARI")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… BAÅARILI" if result else "âŒ BAÅARISIZ"
        print(f"{test_name:20} : {status}")
        if result:
            passed += 1
    
    print(f"\nToplam: {passed}/{total} test baÅŸarÄ±lÄ±")
    
    if passed == total:
        print("ğŸ‰ TÃ¼m testler baÅŸarÄ±lÄ±! Sistem kullanÄ±ma hazÄ±r.")
    elif passed >= total - 1:
        print("âš ï¸ Ã‡oÄŸu test baÅŸarÄ±lÄ±. LLM olmadan da sistem Ã§alÄ±ÅŸabilir.")
    else:
        print("âŒ BirÃ§ok test baÅŸarÄ±sÄ±z. Sistem yapÄ±landÄ±rmasÄ±nÄ± kontrol edin.")
    
    # Ã–neriler
    print("\nğŸ’¡ Ã–NERÄ°LER:")
    if not results.get("LLM EriÅŸilebilirlik", False):
        print("   - Ollama kurulumu yapÄ±n: https://ollama.ai/")
        print("   - Model indirin: ollama pull llama2:7b")
        print("   - Ollama servisini baÅŸlatÄ±n: ollama serve")
    
    if not results.get("ML Model", False):
        print("   - ML model baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kontrol edin")
        print("   - pip install -r ml_clinic/requirements.txt")
    
    if not results.get("Whisper Model", False):
        print("   - Whisper baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kontrol edin")
        print("   - pip install openai-whisper torch torchaudio")

if __name__ == "__main__":
    main()
